微服务下的数据一致性
https://www.jianshu.com/p/b264a196b177
https://www.cnblogs.com/jiangyu666/p/8522547.html
分布式事务的实现方法非常多，比较常见的两种：

1. 小事务 + rpc 调用，调用成功后 -> 小事务 commit， 调用失败 -> 小事务回滚，典型的 2pc 强一致中心化（一个协调节点和 N 个参与者节点）原子提交协议
   缺点：
   同步堵塞问题：任何步骤都需要等待即同步等待，各个参与者在等待其他参与者响应过程中，无法进行其他操作；协作者也需要等待所有的参与者都回复才能进行下一步的动作
   单点问题：协调者只有一个
   脑裂问题：在第二部分中，如果只有部分执行了commit 而另一部分可能因为网络原因没有接受到 commit 命令，将会导致数据不一致

   优点：非常简单，易实现

2. 基于消息的最终一致性方案，通过消息中间件(RabbitMQ、Kafka)保证上、下游应用数据操作的一致性.   
   基本思路是将本地操作和发送消息放在一个事务中，保证本地操作和消息发送要么两者都成功或者都失败，下游应用向消息系统订阅该消息，收到消息后执行相应操作，
   保证消息消费且执行成功
   从本质上讲是将分布式事务转换为两个本地事务，然后依靠下游业务的重试机制达到最终一致性(如果没有 ack 会一直重试，最终失败也有日志记录，最终确保成功)

微服务的缺点：

1. 当服务数量增加，系统管理复杂性增加(就跟拼图游戏一样，切的越碎，越难拼出整副图. 一个系统被拆分成零碎的微服务，最后要集成为一个完成的系统，其复杂度肯定比大块的功能集成要高很多)
2. 分布式系统可能复杂难以管理

需要考虑的问题: 切割问题, 通信问题, 弹性的部署问题, 监控问题
1>切得越碎, 微服务越多, 模块之间的关系可能就越多, 系统的管理复杂性就越高, 所以如何切分是个问题.
2>业务上的你来我往, 这就涉及到要对外通信, 当微服务的数量达到一定量级的时候, 如何提供一个高效的集群通信机制成为一个问题.
3>单个微服务拥有自己的进程, 进程本身就可以动态的启停, 为无缝升级的打好了基础, 但谁来启动和停止进程, 什么时机, 选择在哪台设备上做这件事情才是无缝升级的关键, 而是需要背后强大的版本管理和部署能力
4>多个相同的微服务可以做负载均衡, 以提高性能和可靠性, 正是因为相同微服务可以有多个不同实例, 让服务按需动态伸缩成为可能, 在高峰期可以启动更多的相同的微服务实例为更多用户服务, 但是微服务本身是不会去关心系统负>
5>保障快速审计和跟踪到具体服务, 整个系统状态如何监控.

-----

CAP 理论，关于数据一致性和可用的具体的考虑
我们的理解是做到极致，尽可能的把拆分服务拆分小
需要数据一致的，就用 redis 的分布式锁或者 mysql事务+乐观悲观锁来保证更新的串行化
需要高可用的，就完全抛弃数据一致

CAP定理是分布式系统理论的基础
CAP告诉我们, 对于一个分布式系统, 它无法同时保证一致性(consist)、可用性(aviable)和分区容错性(Partition Tolerance), 而是必须要舍弃其中的一个
对于分布式系统一般我们是不可能舍弃分区容忍性的(因为分区的情况是无法避免的), 所以一般是根据业务, 在一致性和可用性中二选一, C和A二选一

考虑下CAP的原因是什么(场景: 多个tornado实例, 一个存储)
CA一定缺少P: 数据一致&吞吐量高, 那么一定不能用分布式. 考虑它的逆否: 如果你用了分布式, 那么必然涉及到多线程的推进顺序问题和数据的不一致问题
CP一定缺少A: 分布式系统, 数据一致, 那么多线程推进必然有序, 吞吐量一定很低
AP一定缺少C: 一个分布系统并且保证了访问效率, 那么这多个线程之间的互相影响比如数据的争抢就很很不严格, 推进随意所以才会吞吐量高, 这样数据必不一致

春哥说: 在权衡c和a的时候, 应该考虑极限, 或者严格的强一致, 或者随意并发, 并且在需要强一致的地方, 尽可能的把操作做小, 增加吞吐量

-----

BASE 理论
Basically Available 基本可用性: 分布式系统在出现故障的时候, 允许损失部分可用性, 即保证核心可用, 比如熔断, 服务降级, 比如服务隔离
Soft State 软状态: 指允许系统存在中间状态, 在这个状态下数据是不一致的, 比如微服务中基于消息通知的数据一致性方案
Eventual Consistency: 可以有软状态, 但是结果必须是最终一致的

整体可用性的优化: 限流, 熔断, 负载均衡, 服务隔离, 服务发现, 服务降级

最后是做好监控, 和日志收集

-----

3pc
什么是：三阶段提交，是2pc的改进版，其实就是将 2pc 提交事务的过程一分为二了
一般2pc 就是  事务询问，然后各个参数者尝试执行，返回结果，如果协调者发现都成功了则把这个事务提交成 commit
3pc的话就是在当前加了个 询问操作，先询问所有的参与者，是否可以执行，可以了在进行提交尝试执行，后面和 2pc 是一样的
3pc 就是解决了2pc 会堵塞的问题，加入了超时机制在里面，解决了协调者宕机无法释放资源的问题，但是依旧无法解决数据不一致的问题

-----

拜占庭
就是恶意篡改消息导致永远无法达到统一，因为它是口头传播（除非使用签名传播）
这种签名传播的方式一般适用在区块链中

非拜占庭就是允许消息的丢失、重复 但不允许篡改消息

-----

Leases 协议
什么是：一种高效的分布式缓存一致性的高效容错机制，无法解决拜占庭问题，但是能够提升在非拜占庭算法带来的性能影响(每次不去服务器中获取而是在客户端)
前提：必须保证各个机器上的物理时钟是一致的，不然会因为时钟漂移的问题，导致数据错误问题
原理：可以类比成 Java 中的  volatile 关键字，也就是说它可以控制客户端的缓存是否失效。
当有客户端要进行写操作的时候，那么它必须从服务端获取写的权利才能够写，那其他读操作如果本地没有缓存则去服务端获取并缓存到本地，写操作的话会向服务端申请一个latex
这个latex 申请到了才能够有权利进行写操作，并且该客户端需要一直来续约这个latex，分为短租约和长租约
- 短租约相较于长租约来说，可以降低 客户端 crash 时导致的写入延时，还可以降低服务端 crash 时的恢复时间，能够减少伪共享。但是会增加io的成本
- 长租约适用于读多写少的场景下
为什么会降低客户端和服务端的crash，主要是因为写操作的客户端要来服务端获取latex，如果被别的client 获取到了则需要等待它结束，才能获取并写入，写入后会广播出去让其他的
client 相关缓存失效

-----

Gossip 协议
什么是：最终一致性的算法，一般用于在无主的去中心化集群中实现比较多
原理：其实比较简单，就是一个广度优先遍历的算法，假设A得到了某些消息，更新了自身，然后它会告诉周边最近的节点，依次类推，不断的扩散。直至全部感染。
每个节点定期的会随机的抽取几个节点去获取消息消息然后跟自己比对，然后互换双方之间的差异，达到一个最终一致性的效果

-----

一致性Hash
什么是：就是为了拟补 Hash 算法在动态扩容下需要rebalance，以及节点失效的问题
典型的就是Redis 的实现： 2^32 取模，数据顺时针与节点做映射，通过虚拟节点的引入来避免数据分布不均匀避免热点数据

-----

Zab 协议
什么是：最终一致性的算法，一个原子广播的一致性算法协议，比较典型的就是 zk，但是在zk中只有写操作用了zab协议，读操作并没有使用，所以有一个误区zk并不是单纯的强一致的它也可以是最终
一致性的，它有一个 sync 方法就是强一致性的，反之就是最终一致性的
原理： zab 本身是一个最终一致性的算法，首先在写一条消息的时候只需要过半的节点成功就可以了。
消息广播：
1. 客户端发起写请求，leader 节点接受到之后会分配一个全局唯一的 zxid 并且把他封装成一个 proposal
2. leader 节点会为每个 follower 节点分配一个队列，然后把 proposal 分别塞入队列中
3. follower 接受到 proposal 后，会首先将其以事务日志的方式写入本地磁盘中，写入成功后向leader反馈ack
4. leader 接受到过半的ack之后进行commit  
其通过了队列的方式和 follower 进行了解耦，避免了传统 2pc带来的堵塞问题
崩溃恢复：一旦leader crash 或者网络的原因导致 leader 服务器失去了过半follower 的联系，则会进入崩溃恢复模式，这需要重新选择一个新的 leader
1. leader 发出 commit 命令之后，还没等全部follower 都commit 就宕机，这里就会出现 2pc 3pc 的数据不一致的问题
- 这个比较好解决 直接选择 zxid 最大的follower 当leader就行，然后进行数据同步就行
2. leader 刚把 proposol 发出去，但是宕机了
- 这个问题和1其实一样的处理，因为只要 proposol 发出去了，不管是否提交，那么它的zxid 都是被follower 接受到的，
依旧是选择一个最大的 zxid 的follower 成为 leader 然后唯一的区别就是 称为leader 之后先要完成自己没commit 的消息然后继续同步
3. leader 刚接受到消息并封装成 proposol，还没来得及发送就宕机了
- 这个时候这条消息就会被跳过，会丢失的， 这个时候就需要通过客户端配置 进行重发

-----

Paxos 协议
什么是：保证多副本数据强一致的算法，然后它分为几个变种，比如最最基础的那就是 base-paxos 其次 multi paxos 最后 fast paxos
base paxos: 基于多数派的读写，通过 2轮 rpc 来确定一个值
概念：
- Proposer: 发起请求的进程，就可以理解为是 leader
- Acceptor: 接受请求的进程，可以理解为是 follower
- Quorumn: 多数派， n/2 + 1 个 acceptor
- Round: 1轮包含2个阶段 phase1 & phase2
- rnd: 每一轮的编号，单调递增，全局唯一
原理：
1. proposer 发送请求并把 rnd 设置为 1，当accpetor 接受到请求之后，进行判断是否比自己的rnd 小，小则拒绝，大则覆盖，然后返回自己的rnd 和之前已接受到的值
2. proposer 收到acceptor 的响应之后判断 rnd 是否比自己大，是的话退出，因为发生了竞争，这里其实有两种情况：
- 所有返回的 acceptor 中所带的上一次的值都是空，那么说明系统是非常干净的，则可以把自己想要写的值，进行第二阶段
- 但是如果 acceptor 中有值，则说明有其他的proposer 也在操作，则选择最大的值，进入第二阶段
- 并且如果没有收到过半的 acceptor 则退出
3. 开始第二阶段，propose 开始把选定的值发给 acceptor，这个值可能是自己要写入的，也可能是帮别的proposor 进行修复的
注意一个点：如果在开始第二阶段之前，有并发的 proposer 写入了一个 较大的 rnd 则这次操作会失败，要进行重试
4. 然后 acceptor 还是会比较接受的 rnd 是否与自己一致，一致则写入

multi paxos: 通过 1轮 rpc 来确定一个值
把多个 paxos实例 合并发送 batch
fast paxos: 没有冲突则一轮，有冲突则两轮
增加quorum 的数量来达到一致的目的，它是直接进行阶段2，但是在阶段2中出现了任何的冲突立马会回退成base paxos 其实效率更差

-----

Raft 协议
什么是：保证多副本数据强一致的算法，其实也是本质上也是 paxos，这是在 paxos 中当入了状态机，更加的让人能够理解，其核心就是 multi paxos 的一个应用
原理：首先 raft 有 leader follower candidate 三个角色，所有节点一开始都是 follower 候选者状态，然后会为每个节点随机生成一个过期时间，当 follower
过期之后立马会升级成 candidate 状态，一旦到 candidate 状态之后立马会发送 election 请求给其他节点进行 leader 的选举，然后呢每个follower 都会与之
比较term(epoch)如果比自己小，说明发生了脑裂直接丢弃，反正比较我大则响应，收到过半的响应之后升级成leader 节点。
然后leader 节点会定期的发送心跳过去告诉follower 节点，不需要转变成candidate 状态，所以在实现的过程中，我们自己定义的发送心跳时间要比follower 这个过期时间要断，
不然就会出现leader 还没发送心跳过去，follower 就转换成 candidate 进行leader 选举了。
然后leader 就开始把客户端的消息封装，会包含 自己的 term 编号以及log的顺序索引发送过去。这个会出现数据不一致的问题，那就是当leader 发送消息出去，然后宕机，并没有完全的发送给所有的 follower.

     Raft 的做法是，选举 Leader 节点除了判断 term 的大小之外还需要满足log 条目要和大部分的follower 节点一样新才可以能成leader
     成为 leader 之后，最向follower 发送自己的offset 逐步的递减来获取最新一致的日志，然后进行同步，之后会要求所有的follower 和leader 同步重写截断leader 不存在的数据
     这个时候必定会有数据丢失，所以需要客户端进行重发

优化：可以通过客户端批量写入，采用 leases read 客户端缓存，或者 follower read
但是如果要实现  follower read，必须满足单调读，不然会导致读写数据不一致问题
所以这个时候会去 leader 节点请求 commit index 然后在去follower 节点判断是否大于等于readindex

1. Multi-Group Raft
2. Join Consensus
3. Batch and Pipeline & Append Log Parallelly & Asynchrouous Apply
4. LogRead & ReadIndex & Leases Read & Follower Read

-----

关于分布式事务

TCC
什么是：try-confirm-cancel 其模式本质上来将就是 2pc，只是它是在业务上进行实现的
- 一阶段 prepare行为；调用自定义的prepare 逻辑
- 二阶段 commit 行为：调用自定义的 commit 逻辑
- 二阶段 rollback 行为：调用自定义的 rollback 逻辑

XA
什么是：XA 主要定义了事务管理器 TM 和局部资源管理器 RM 之间的接口，目前主流的数据库 oracle、 mysql 都支持
其实就是 3pc 的一个操作，相比较与 saga 的话就是可以对业务无入侵

Saga
什么是：主要用来解决一些长事务，业务流程中每个参与者都提交本地事务，当出现某一个参与者失败则补偿前面已经成功的参与者，一阶段正向服务和二阶段服务都是由业务开发实现
需要一个协调组件

Percolator
什么是：

-----

不能用 zk 存储海量数据的原因
znode 树维系在内存中，并且多个zk 存储的是相同的数据造成内存浪费
zk 使用分布式协调的而不是用来做分布式锁的

-----

分布式事务了解吗？你们项目中都用到了哪些分布式事务？都有哪些优缺点？
一般常用的是还是 2pc 的方式 也就是tcc，适合那些强一致的场景，在io环境不好、或者对同一个数据竞争激烈的情况下会堵塞 性能较差
还有一种就是 消息队列，适用于对数据实时性不是很敏感的那种情况，而且用消息队列可以做到解耦

